{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9015118b-cd07-49e0-ae90-1e57d5a95768",
   "metadata": {},
   "source": [
    "<h1>ML Assignment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cdc7ab-e029-4c6b-8754-40ec926411fd",
   "metadata": {},
   "source": [
    "<h2>Install</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fdb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "#!pip install wordcloud\n",
    "#!pip install scattertext\n",
    "#!pip install textblob\n",
    "#!pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e12e3f-5aff-4691-9e7f-68a6fb331eec",
   "metadata": {},
   "source": [
    "<h2>Import</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0c73a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "657b8c0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tqz11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import collections\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from textblob import TextBlob\n",
    "from scipy.stats import kde\n",
    "import networkx as nx\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('wordnet')\n",
    "# Ngrams allows to group words in common pairs or trigrams..etc\n",
    "from nltk import ngrams\n",
    "# We can use counter to count the objects\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a09af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf4eafa-a51d-4abe-8904-67c7a307d5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections.abc import Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9349707-986d-499e-a571-da78c0077346",
   "metadata": {},
   "source": [
    "<h2>Input data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80ea0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "# Read the CSV file with specified column names\n",
    "df = pd.read_csv(\"X_dataset.csv\", encoding=\"ISO-8859-1\", names=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17656a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b501e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting target and features\n",
    "text = df[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a2f14a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Series name: text\n",
      "Non-Null Count    Dtype \n",
      "--------------    ----- \n",
      "1600000 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 12.2+ MB\n"
     ]
    }
   ],
   "source": [
    "text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77ced3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000,)\n"
     ]
    }
   ],
   "source": [
    "print(text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4d6de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e79833e4-7b09-4af8-8e48-b994a739a67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @switchfoot http://twitpic.com/2y1zl - awww, t...\n",
       "1    is upset that he can't update his facebook by ...\n",
       "2    @kenichan i dived many times for the ball. man...\n",
       "3      my whole body feels itchy and like its on fire \n",
       "4    @nationwideclass no, it's not behaving at all....\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e82eb1-090e-45fc-b3d2-ddae4056e4ae",
   "metadata": {},
   "source": [
    "<h2>Remove Email</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a9a7e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_email(df):\n",
    "    return re.sub('@[^\\s]+', ' ', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5ecef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.apply(lambda x: cleaning_email(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a43cb387-9c2b-4fbd-be2c-13070dddfd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      http://twitpic.com/2y1zl - awww, that's a bu...\n",
       "1    is upset that he can't update his facebook by ...\n",
       "2      i dived many times for the ball. managed to ...\n",
       "3      my whole body feels itchy and like its on fire \n",
       "4      no, it's not behaving at all. i'm mad. why a...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acd0c6-ba9e-4292-a6bc-eea3a09c7cc2",
   "metadata": {},
   "source": [
    "<h2>Remove URL</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ae9088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_URLs(df):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4275f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.apply(lambda x: cleaning_URLs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c57bbef-80a6-4ae4-884e-18fbeab0fcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        - awww, that's a bummer.  you shoulda got ...\n",
       "1    is upset that he can't update his facebook by ...\n",
       "2      i dived many times for the ball. managed to ...\n",
       "3      my whole body feels itchy and like its on fire \n",
       "4      no, it's not behaving at all. i'm mad. why a...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e5edf-8d6a-4b41-90a7-c464ac0ac03e",
   "metadata": {},
   "source": [
    "<h2>Punctuation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f6f72f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_punctuations = string.punctuation\n",
    "english_punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "674b4565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', english_punctuations)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b75b82-10f2-4057-a006-141bce5bf9ce",
   "metadata": {},
   "source": [
    "<h2>Remove Punctuation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2f846c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.apply(lambda text: cleaning_punctuations(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbce3454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         awww thats a bummer  you shoulda got davi...\n",
       "1    is upset that he cant update his facebook by t...\n",
       "2      i dived many times for the ball managed to s...\n",
       "3      my whole body feels itchy and like its on fire \n",
       "4      no its not behaving at all im mad why am i h...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c740f87-e85c-4dce-a554-e6834b575338",
   "metadata": {},
   "source": [
    "<h2>Remove Numbers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cfffb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2fc2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.apply(lambda x: cleaning_numbers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17bc90fa-da49-487b-a1cb-46d2be2132e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         awww thats a bummer  you shoulda got davi...\n",
       "1    is upset that he cant update his facebook by t...\n",
       "2      i dived many times for the ball managed to s...\n",
       "3      my whole body feels itchy and like its on fire \n",
       "4      no its not behaving at all im mad why am i h...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411066ee-f518-4b16-9aef-adf2f8fca403",
   "metadata": {},
   "source": [
    "<h2>Sampel Stopwords</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7e310e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f424245-2da5-41fa-9c46-f129477203dc",
   "metadata": {},
   "source": [
    "<h2>Remove Stop Words</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d716aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords_list)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4c8c783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = text.apply(lambda text: cleaning_stopwords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f74b2f6-5514-42e3-8443-74ebead2a4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    awww thats bummer shoulda got david carr third...\n",
       "1    upset cant update facebook texting might cry r...\n",
       "2    dived many times ball managed save rest go bounds\n",
       "3                     whole body feels itchy like fire\n",
       "4                             behaving im mad cant see\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4515bf64-403b-46c7-bedc-0bd79b25f3cb",
   "metadata": {},
   "source": [
    "<h2>Tokenization</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cc3aeac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "text = text.apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1810715-c37a-4564-8578-463fe1b5fba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [awww, thats, bummer, shoulda, got, david, car...\n",
       "1    [upset, cant, update, facebook, texting, might...\n",
       "2    [dived, many, times, ball, managed, save, rest...\n",
       "3              [whole, body, feels, itchy, like, fire]\n",
       "4                       [behaving, im, mad, cant, see]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "415900fe-1878-4dc0-8667-23f948fee6f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awww',\n",
       " 'thats',\n",
       " 'bummer',\n",
       " 'shoulda',\n",
       " 'got',\n",
       " 'david',\n",
       " 'carr',\n",
       " 'third',\n",
       " 'day',\n",
       " 'upset',\n",
       " 'cant',\n",
       " 'update',\n",
       " 'facebook',\n",
       " 'texting',\n",
       " 'might',\n",
       " 'cry',\n",
       " 'result',\n",
       " 'school',\n",
       " 'today',\n",
       " 'also',\n",
       " 'blah',\n",
       " 'dived',\n",
       " 'many',\n",
       " 'times',\n",
       " 'ball',\n",
       " 'managed',\n",
       " 'save',\n",
       " 'rest',\n",
       " 'go',\n",
       " 'bounds',\n",
       " 'whole',\n",
       " 'body',\n",
       " 'feels',\n",
       " 'itchy',\n",
       " 'like',\n",
       " 'fire',\n",
       " 'behaving',\n",
       " 'im',\n",
       " 'mad',\n",
       " 'cant',\n",
       " 'see',\n",
       " 'whole',\n",
       " 'crew',\n",
       " 'need',\n",
       " 'hug',\n",
       " 'hey',\n",
       " 'long',\n",
       " 'time',\n",
       " 'see',\n",
       " 'yes',\n",
       " 'rains',\n",
       " 'bit',\n",
       " 'bit',\n",
       " 'lol',\n",
       " 'im',\n",
       " 'fine',\n",
       " 'thanks',\n",
       " 'hows',\n",
       " 'nope',\n",
       " 'didnt',\n",
       " 'que',\n",
       " 'muera']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_visual = text[:10]\n",
    "normal_visual = [word for i in normal_visual for word in i]\n",
    "normal_visual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf23f9-eb5f-4a01-a354-1e997cdf906c",
   "metadata": {},
   "source": [
    "<h2>Lemmatizer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b28aa3d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lemmatizer_on_text(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90ba96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.apply(lambda x: lemmatizer_on_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c697185-823c-4953-8d7b-dfb5ed7ef531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [awww, thats, bummer, shoulda, got, david, car...\n",
       "1    [upset, cant, update, facebook, texting, might...\n",
       "2    [dived, many, time, ball, managed, save, rest,...\n",
       "3               [whole, body, feel, itchy, like, fire]\n",
       "4                       [behaving, im, mad, cant, see]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c021530-621c-420e-b892-78153687b8fc",
   "metadata": {},
   "source": [
    "# Function to remove nouns using POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eedb26e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove nouns using POS tagging\n",
    "def remove_nouns(tokenized_words):\n",
    "    tagged_words = nltk.pos_tag(tokenized_words)\n",
    "    words_without_nouns = [word for word, tag in tagged_words if tag != 'NN' and tag != 'NNS']\n",
    "    return words_without_nouns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b2db2cf-eeef-4ec3-a223-4c53a4135e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time is: 15:50:36\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\tqz11/nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent Time is:\u001b[39m\u001b[38;5;124m'\u001b[39m, current_time)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Remove nouns and recombine words\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWithout_Nouns_data_normal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_nouns\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecombined_data_normal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWithout_Nouns_data_normal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m y: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(y) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, Iterable) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# storing the current time in the variable\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent Time is:\u001b[39m\u001b[38;5;124m'\u001b[39m, current_time)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Remove nouns and recombine words\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWithout_Nouns_data_normal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m y: \u001b[43mremove_nouns\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecombined_data_normal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWithout_Nouns_data_normal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m y: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(y) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, Iterable) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# storing the current time in the variable\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[35], line 3\u001b[0m, in \u001b[0;36mremove_nouns\u001b[1;34m(tokenized_words)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_nouns\u001b[39m(tokenized_words):\n\u001b[1;32m----> 3\u001b[0m     tagged_words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     words_without_nouns \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word, tag \u001b[38;5;129;01min\u001b[39;00m tagged_words \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m tag \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNNS\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m words_without_nouns\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPICKLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:522\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Check each item in our path\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path_ \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;66;03m# Is the path item a zipfile?\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path_ \u001b[38;5;129;01mand\u001b[39;00m (\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    523\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    524\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ZipFilePathPointer(path_, resource_name)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " \n",
    "# storing the current time in the variable\n",
    "c = datetime.now()\n",
    "# Displays Time\n",
    "current_time = c.strftime('%H:%M:%S')\n",
    "print('Current Time is:', current_time)\n",
    "\n",
    "# Remove nouns and recombine words\n",
    "df['Without_Nouns_data_normal'] = text.apply(lambda y: remove_nouns(y))\n",
    "df['Recombined_data_normal'] = df['Without_Nouns_data_normal'].apply(lambda y: ' '.join(y) if isinstance(y, Iterable) else '')\n",
    "\n",
    "# storing the current time in the variable\n",
    "c = datetime.now()\n",
    "# Displays Time\n",
    "current_time = c.strftime('%H:%M:%S')\n",
    "print('Current Time is:', current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc73bc2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_pos = df['Recombined_data_normal']\n",
    "text_pos.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ace4ab2",
   "metadata": {},
   "source": [
    "# Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c9eab0-e894-466c-ab60-78f688ec8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=10000\n",
    "\n",
    "# Perform simple random sampling\n",
    "samplepos = text_pos.sample(n=sample_size, random_state=42)  # Set random_state for reproducibility\n",
    "\n",
    "\n",
    "# Define the interval for systematic sampling\n",
    "interval = len(normal) // sample_size\n",
    "\n",
    "# Choose a random start within the interval\n",
    "start = np.random.randint(0, interval)\n",
    "\n",
    "# Perform systematic sampling\n",
    "normal_RS = normal.iloc[start::interval]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f00b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = normal_RS[\"text\"]\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a538f9f-0223-46b1-ab3b-9d980ad6cca2",
   "metadata": {},
   "source": [
    "# Text convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea75ff-2407-4a45-bb4e-8ed9f73f9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a30755-b4a1-4e3b-becd-9d85bc901244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6e72d1",
   "metadata": {},
   "source": [
    "## Applying K Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b5529-44a6-4002-b527-731ad3a30b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-means clustering\n",
    "num_clusters = 2  # You can choose the number of clusters based on domain knowledge or using techniques like the elbow method\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe22ac-0013-4554-8be0-84e319727f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get cluster labels\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1635f5-d05e-4165-b81a-ea1d89f326a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "YN = input(\"Do you want to show Cluster 1 or Cluster 2 (one/two):\")\n",
    "\n",
    "\n",
    "if YN == \"one\":\n",
    "    # Analyze clusters\n",
    "    for i in range(num_clusters):\n",
    "        cluster_normal_clr = [normal_clr[j] for j in range(len(normal_clr)) if cluster_labels[j] == i]\n",
    "        print(f\"Cluster {i + 1}:\")\n",
    "        if i == 0:\n",
    "            for text in cluster_normal_clr:\n",
    "                print(text)\n",
    "            print()\n",
    "        else:\n",
    "            print(\"NO\")\n",
    "elif YN == \"two\":\n",
    "    # Analyze clusters\n",
    "    for i in range(num_clusters):\n",
    "        cluster_normal_clr = [normal_clr[j] for j in range(len(normal_clr)) if cluster_labels[j] == i]\n",
    "        print(f\"Cluster {i + 1}:\")\n",
    "        if i == 1:\n",
    "            for text in cluster_normal_clr:\n",
    "                print(text)\n",
    "            print()\n",
    "        else:\n",
    "            print(\"NO\")\n",
    "else:\n",
    "    print(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380d66c-80cd-4e86-9e00-5734142b1238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# storing the current time in the variable\n",
    "c = datetime.now()\n",
    "# Displays Time\n",
    "current_time = c.strftime('%H:%M:%S')\n",
    "print('Current Time is:', current_time)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate clustering quality (optional)\n",
    "silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "print(\"Silhouette Score:\", silhouette_avg)\n",
    "\n",
    "\n",
    "\n",
    "# storing the current time in the variable\n",
    "c = datetime.now()\n",
    "# Displays Time\n",
    "current_time = c.strftime('%H:%M:%S')\n",
    "print('Current Time is:', current_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d0710-00dc-48bf-9116-bea4bfdab5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Apply dimensionality reduction for visualization (optional)\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "X_reduced = svd.fit_transform(X)\n",
    "\n",
    "# Plot clusters\n",
    "for i in range(num_clusters):\n",
    "    cluster_texts = [normal_clr[j] for j in range(len(normal_clr)) if cluster_labels[j] == i]\n",
    "    cluster_x = X_reduced[cluster_labels == i, 0]\n",
    "    cluster_y = X_reduced[cluster_labels == i, 1]\n",
    "    plt.scatter(cluster_x, cluster_y, label=f'Cluster {i + 1}')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('K-means Clustering of Text Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot centroids\n",
    "centroids_reduced = svd.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centroids_reduced[:, 0], centroids_reduced[:, 1], marker='x', s=100, c='red', label='Centroids')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f8ece-d1f5-47ee-abde-69012847de85",
   "metadata": {},
   "source": [
    "# Agglomerative Hierarchical Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4839b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2697b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of clusters\n",
    "num_clusters = 2\n",
    "\n",
    "# Create an instance of AgglomerativeClustering\n",
    "hierarchical_cluster = AgglomerativeClustering(n_clusters=num_clusters)\n",
    "\n",
    "# Fit the model and predict the labels\n",
    "labels = hierarchical_cluster.fit_predict(X_reduced)\n",
    "\n",
    "# Plot clusters\n",
    "for i in range(num_clusters):\n",
    "    cluster_texts = [normal_clr[j] for j in range(len(normal_clr)) if cluster_labels[j] == i]\n",
    "    cluster_x = X_reduced[cluster_labels == i, 0]\n",
    "    cluster_y = X_reduced[cluster_labels == i, 1]\n",
    "    plt.scatter(cluster_x, cluster_y, label=f'Cluster {i + 1}')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('K-means Clustering of Text Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot centroids\n",
    "centroids_reduced = svd.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centroids_reduced[:, 0], centroids_reduced[:, 1], marker='x', s=100, c='red', label='Centroids')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de70598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe9edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3337316b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f9c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb4783d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c9db6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf7908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b4d9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede5590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d78f580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90753a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d63ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ffaed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1588426d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748dbed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a15d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f764f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cf7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b27f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71c598c4-ac38-4125-ad3e-5c966122779c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h2>Word Cloud</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc0f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the text for the Word Cloud\n",
    "wordcloud_text = data.str.cat(sep=' ')\n",
    "# Size of Word Cloud # (max_font_size = 100, max_words = 50,)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "# This use to import image and apply to the Word Cloud\n",
    "custom_mask = np.array(Image.open('twitter_wordcloud.png'))\n",
    "# Make Wordcloud\n",
    "wordcloud = WordCloud(background_color = \"white\", colormap = 'plasma', mask = custom_mask).generate(wordcloud_text)\n",
    "\n",
    "# Plot Wordcloud\n",
    "plt.plot()\n",
    "plt.imshow(wordcloud, interpolation = \"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd21ec7-1583-4315-a481-fbbb59e9a4f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = collections.Counter(wordcloud_text.split())\n",
    "res_mostcommon = res.most_common(10)\n",
    "res_mostcommon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453dcce4-fbc0-4a2a-a305-38c24c5b8f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, counts = zip(*res_mostcommon)\n",
    "values2,counts2 = zip(*res.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b17622-2ca8-46e5-bb2d-bd64d9123fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Create a radar chart\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "      r=counts,\n",
    "      theta=values,\n",
    "      fill='toself',\n",
    "      name='Word'\n",
    "))\n",
    "# Add title\n",
    "fig.update_layout(title='Radar Chart', width=800, height=800)\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e5e5b-16b2-44a3-93f0-cdb311dafa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.bar(values, counts, color ='maroon', \n",
    "        width = 0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6edade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=values,\n",
    "        y=counts\n",
    "    ))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=values,\n",
    "        y=counts\n",
    "    ))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda6315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084cc7a-a3f1-45e5-bf16-5e4736df65b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c265303-9cb6-4b6a-a7fc-cc5afe32f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment polarity for each comment\n",
    "data_sentiment = data.apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming data_sentiment is a\n",
    "# Create the KDE object\n",
    "data_density = kde.gaussian_kde(data_sentiment)\n",
    "\n",
    "# Generate points for the density curve\n",
    "x = np.linspace(data_sentiment.min(), data_sentiment.max(), 100)\n",
    "density = data_density(x)\n",
    "\n",
    "plt.plot(x, density, label='Sentiment Distribution')\n",
    "plt.xlabel('Sentiment Polarity')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Sentiment Distribution in data_sentiment')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe1dca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f194e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092e683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f210d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bigrams from the 'text' column of the DataFrame\n",
    "bigrams = list(ngrams(data2, 2))\n",
    "bigram_counts = Counter(bigrams)\n",
    "\n",
    "# Get the top 5 most common bigrams\n",
    "top_5_bigrams = bigram_counts.most_common(5)\n",
    "\n",
    "# Create a new graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges with weights for all bigrams\n",
    "for bigram, count in bigram_counts.items():\n",
    "    G.add_edge(bigram[0], bigram[1], weight=count)\n",
    "\n",
    "# Extract just the bigrams from the top 10 for highlighting\n",
    "top_bigrams = [bigram for bigram, count in top_5_bigrams]\n",
    "\n",
    "# Set node sizes based on whether the bigram is in the top 5\n",
    "node_sizes = [1000 if node in top_bigrams else 100 for node in G.nodes()]\n",
    "\n",
    "# Explicitly create a figure and axes object\n",
    "plt.figure(figsize=(12, 8))  # Optional: specify the size of the figure\n",
    "ax = plt.gca()  # Get the current axes\n",
    "\n",
    "# Draw the graph with highlighted nodes for the top 5 bigrams\n",
    "pos = nx.spring_layout(G)  # Positioning of nodes\n",
    "nx.draw(G, pos, ax=ax, node_color='lightblue', node_size=node_sizes, with_labels=True)\n",
    "\n",
    "# Finally, display theplot\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
