{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a720fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install easygui\n",
    "#!pip install pypdf\n",
    "#!pip install flask\n",
    "#!pip install tk\n",
    "#!pip install PyPDF2\n",
    "#!pip install tkPDFViewer\n",
    "#!pip install spacy \n",
    "#! python -m spacy download en_core_web_sm\n",
    "#!pip install pdfquery\n",
    "#!pip install -U spacy      #<--- (cmd) package\n",
    "#!python -m spacy download en    #<--- language model\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install spacy\n",
    "#!{sys.executable} -m spacy download en\n",
    "\n",
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "#!pip install sentencepiece\n",
    "#!pip install Spire.Doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad1e31e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import easygui \n",
    "\n",
    "import os\n",
    "\n",
    "import string\n",
    "import re \n",
    "\n",
    "import sys\n",
    "\n",
    "import pdfquery\n",
    "from pypdf import PdfReader \n",
    "from pdfquery import PDFQuery\n",
    "\n",
    "import flask\n",
    "\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import spacy \n",
    "\n",
    "import requests \n",
    "from spacy import displacy\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tkinter import *\n",
    "from tkinter import messagebox \n",
    "from tkinter import ttk\n",
    "from tkinter import filedialog\n",
    "from tkinter.filedialog import askopenfile\n",
    "from tkinter import simpledialog\n",
    "from tkPDFViewer import tkPDFViewer as pdf \n",
    "from PIL import Image, ImageTk\n",
    "import PyPDF2\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fdcf8c-19c6-4696-939d-0820220e757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable to disable the warning\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "805f7548-99e6-4768-980d-4f920ce3313f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To: rashidah@university.edu.my From: abdulrazak@yoyo.com.my Subject: Apology for not attending lectures Dear Dr Rashidah, Thank you for your email regarding my absence from class. My friend told me that you had written to me a few days ago. Unfortunately, there is something wrong with my student email and I have not been able to use it since January 2021. I am now writing to you using my personal email. I would like to explain my absence from your lectures for the past two weeks and for having not informed you earlier. I was unwell on 17 February 2021. However, I did not consult a doctor because I was too weak and took a painkiller instead. As a result, I did not have a medical certificate to hand in. On 24 February 2021, I could not come to class because I had to attend my sister’s wedding in my village. In fact, I had asked my friend who was in the same class to inform you, but she forgot and went to the library to meet friends. For your information, I had planned to come back to the university immediately after my sister’s wedding. But, there was heavy rain and my journey took longer than usual and I could not reach the airport on time. I missed my flight and could only come back the next day. I am aware that I missed a listening test held in class on 24 February 2021. Can I be given the opportunity to take the test because I might fail the course if I do not get any marks for the test? Could I possibly take the test on Friday morning because I have an important oral presentation for another course in the afternoon? Alternatively, could I do a project to replace this test? I hope you can consider my request to take the test that I missed. I apologise again for my behaviour and I will learn to be more responsible. I promise to attend all your classes in future. In the event I cannot attend a class, I will email to inform you. I look forward to hearing from you. Thank you. Sincerely, Abdul Razak'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc682b82-c258-4f8a-bd30-d39dd258ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name1 = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp1 = pipeline('question-answering', model=model_name1, tokenizer=model_name1)\n",
    "\n",
    "userinput1 = 'where is the location?'\n",
    "\n",
    "res1 = nlp(userinput1, text)\n",
    "print(res1)\n",
    "\n",
    "print(res1['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649451c5-5d20-4b55-a9b0-e12ca956bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/tinyroberta-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "\n",
    "userinput = 'what he promising'\n",
    "\n",
    "\n",
    "res = nlp(userinput, text)\n",
    "print(res)\n",
    "\n",
    "print(res['answer'])\n",
    "                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9613fe-c78b-467c-b7c9-393061a4ba74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'I was unwell on 17 February 2021. However, I did not consult a doctor because I was too weak and took a painkiller instead. On 24 February 2021, I could not come to class because I had to attend my sister’s wedding in my village. Can I be given the opportunity to take the test because I might fail the course if I do not get any marks for the test?'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "print(summarizer(text, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557d32cf-a4ae-401b-bc83-a128e07bec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Config, T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model_name = \"allenai/t5-small-squad2-question-generation\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def run_model(input_string, **generator_args):\n",
    "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    res = model.generate(input_ids, **generator_args)\n",
    "    output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "    print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7472c180-e4d7-4a26-b9b1-251e37bc737a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What was the name of the student email that I received from you?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What was the name of the student email that I received from you?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "421eb97a-664a-49e6-a644-87c3b635928e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  What did I take instead of a doctor?\n",
      "answer:  a painkiller\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "question_answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "question_answer = question_answer.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
    "question, answer = question_answer.split(tokenizer.sep_token)\n",
    "\n",
    "print(\"question:\", question)\n",
    "print(\"answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829dfd7-424f-4582-b109-0f988aa95530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11dfb32-5053-43fe-bd8f-4fc6895b6738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84558d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5808\\812095352.py\", line 29, in openfile\n",
      "    file_name = split_tup[0]\n",
      "                ^^^^^^^^^\n",
      "NameError: name 'split_tup' is not defined\n"
     ]
    }
   ],
   "source": [
    "pages = []\n",
    "text = \"\"\n",
    "summary = \"\"\n",
    "#===========\n",
    "root = Tk()\n",
    "root.geometry('700x500+400+100')\n",
    "#===========\n",
    "icon = PhotoImage(file = \"pdf_icon.png\")\n",
    "root.iconphoto(False, icon)\n",
    "root.configure(bg=\"white\")\n",
    "root.title(\"PDF\")\n",
    "#===========\n",
    "root.columnconfigure(0, weight = 1)\n",
    "root.columnconfigure(1, weight = 3)\n",
    "root.rowconfigure(0, weight = 1)\n",
    "#===========\n",
    "# Create text widget and specify size.\n",
    "right_content = Text(root)\n",
    "right_content.grid(row = 0, column = 1)\n",
    "\n",
    "T = Text(root)\n",
    "T.grid(row = 0, column = 0)\n",
    "\n",
    "\n",
    "# Read File\n",
    "def openfile():\n",
    "    filename = filedialog.askopenfilename(initialdir=\"/Documents\", title=\"Upload a PDF file\", filetype=[(\"pdf files\", \"*.pdf\")])\n",
    "    # extract the file name and extension\n",
    "    file_name = split_tup[0]\n",
    "    file_extension = split_tup[1]\n",
    "     \n",
    "    print(\"File Name: \", file_name)\n",
    "    print(\"File Extension: \", file_extension)\n",
    "    \n",
    "    global text\n",
    "    text_output = \"\"\n",
    "\n",
    "    if file_extension == \".pdf\":\n",
    "        \n",
    "          \n",
    "        # creating a pdf reader object \n",
    "        pdf = pdfquery.PDFQuery(filename)\n",
    "        pdf.load()\n",
    "        text = pdf.pq('LTTextLineHorizontal').text()\n",
    "        T.insert(END, text)\n",
    "        # text = textpreprocessing(text)\n",
    "        print(text)\n",
    "    elif file_extension == \".docx\":\n",
    "        print(file_extension)\n",
    "\n",
    "    \n",
    "def textpreprocessing(text):\n",
    "    \n",
    "    lower_text = text.lower()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokenize_text = word_tokenize(lower_text)\n",
    "\n",
    "    # Define punctuation marks to remove (excluding full stop)\n",
    "    punctuation_to_remove = ''.join([char for char in string.punctuation if char != '.'])\n",
    "\n",
    "    # Remove punctuation marks except for full stop\n",
    "    cleaned_words = [word for word in tokenize_text if word not in punctuation_to_remove]\n",
    "\n",
    "    # remover = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    # text_tokens = remover.tokenize(space_removal_text)\n",
    "\n",
    "    #NLTK STOP WORD\n",
    "    text_stopwords = [word for word in cleaned_words if not word in stopwords.words()]\n",
    "\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    lemmatized_content = []\n",
    "\n",
    "    for word in text_stopwords:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        lemmatized_content.append(lemmatized_word)\n",
    "    \n",
    "    # Joining the lemmatized words back into a string\n",
    "    lemmatized_text = ' '.join(lemmatized_content)\n",
    "    return lemmatized_text \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def summary():\n",
    "    global text, summary\n",
    "    print(\"\\n summary \\n\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Calculate the importance score for each sentence based on the sum of token weights\n",
    "    sentence_importance = {}\n",
    "    for sent in doc.sents:\n",
    "        sentence_importance[sent] = sum(token.vector_norm for token in sent if not token.is_stop)\n",
    "    \n",
    "    # Sort sentences by importance score\n",
    "    sorted_sentences = sorted(sentence_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Set the number of sentences for the summary\n",
    "    num_sentences = 5  # You can adjust this number as needed\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary = \" \".join(sent.text for sent, _ in sorted_sentences[:num_sentences])\n",
    "    \n",
    "    # Print the summary\n",
    "    print(summary)\n",
    "\n",
    "\n",
    "def QandA():\n",
    "    global text, summary\n",
    "    userinput = simpledialog.askstring('Q&A', 'What you want to ask?')\n",
    "\n",
    "    userinput = str(userinput)\n",
    "    summary = str(text)\n",
    "\n",
    "    model_name = 'deepset/roberta-base-squad2'\n",
    "    \n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "#     question_answerer = pipeline('question-answering', model=model_name,)\n",
    "#     question_answerer[{'question': userinput, 'context': text}]\n",
    "    \n",
    "    inputs0 = tokenizer(userinput, summary, return_tensors=\"pt\")\n",
    "    output0 = model(**inputs0)\n",
    "\n",
    "    \n",
    "    answer_start_idx = torch.argmax(output0.start_logits)\n",
    "    anser_end_idx = torch.argmax(output0.end_logits)\n",
    "    \n",
    "    answer_tokens = inputs0.input_ids[0, answer_start_idx: anser_end_idx + 1]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "    messagebox.showinfo(\"OutPut\", \"ques: {} \\n answer: {}\".format(userinput, answer))\n",
    "\n",
    "    print(\"ques: {} \\n answer: {}\".format(userinput, answer))\n",
    "    \n",
    "\n",
    "# Define a function to close the window\n",
    "def close():\n",
    "   root.destroy()\n",
    "\n",
    "\n",
    "\n",
    "#Create a Menu\n",
    "my_menu= Menu(root)\n",
    "root.config(menu=my_menu)\n",
    "#Add dropdown to the Menus\n",
    "file_menu=Menu(my_menu,tearoff=False)\n",
    "my_menu.add_cascade(label=\"File\",menu= file_menu)\n",
    "file_menu.add_command(label=\"Open\",command= openfile)\n",
    "file_menu.add_command(label=\"Clear\")\n",
    "file_menu.add_command(label=\"Quit\",command= close)\n",
    "\n",
    " \n",
    "# Create a Button\n",
    "button1 = Button(root, text = 'Summary', bd = '5', command = summary) \n",
    "button1.grid(row = 0, column = 1)\n",
    "\n",
    "button2 = Button(root, text = 'Q&A', bd = '5', command = QandA) \n",
    "button2.grid(row = 0, column = 2)\n",
    "\n",
    "root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492905f-0e6b-43b2-9ed3-b4498fd7dc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "text = \"\"\n",
    "#===========\n",
    "root = Tk()\n",
    "root.geometry('700x500+400+100')\n",
    "#===========\n",
    "icon = PhotoImage(file = \"pdf_icon.png\")\n",
    "root.iconphoto(False, icon)\n",
    "root.configure(bg=\"white\")\n",
    "root.title(\"PDF\")\n",
    "#===========\n",
    "root.columnconfigure(0, weight = 1)\n",
    "root.columnconfigure(1, weight = 3)\n",
    "root.rowconfigure(0, weight = 1)\n",
    "#===========\n",
    "# Create text widget and specify size.\n",
    "right_content = Text(root)\n",
    "right_content.grid(row = 0, column = 1)\n",
    "\n",
    "T = Text(root)\n",
    "T.grid(row = 0, column = 0)\n",
    "\n",
    "#frm = ttk.Frame(root, padding=10)\n",
    "#frm.grid()\n",
    "\n",
    "# Read File\n",
    "def openfile():\n",
    "    global text\n",
    "    text_output = \"\"\n",
    "    filename = filedialog.askopenfilename(initialdir=\"/Documents\", title=\"Upload a PDF file\", filetype=[(\"pdf files\", \"*.pdf\")])\n",
    "      \n",
    "    # creating a pdf reader object \n",
    "    pdf = pdfquery.PDFQuery(filename)\n",
    "    pdf.load()\n",
    "    text = pdf.pq('LTTextLineHorizontal').text()\n",
    "    T.insert(END, text)\n",
    "    textpreprocessing(text)\n",
    "    print(text)\n",
    "\n",
    "    \n",
    "def textpreprocessing(text):\n",
    "    #nlp = spacy.load(\"en_core_web_sm\")\n",
    "    #pd.set_option(\"display.max_rows\", 200)\n",
    "    #doc = nlp(text)\n",
    "    #displacy.render(doc, style=\"ent\")\n",
    "    \n",
    "    #for ent in doc.ents:\n",
    "    #    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Punctuation Handling (all exclamation points (!) to periods (.))\n",
    "    text = text.replace('!', '.')\n",
    "    #text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # White space removal\n",
    "    text = text.strip()\n",
    "\n",
    "    \n",
    "    remover = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    text_tokens = remover.tokenize(text)\n",
    "\n",
    "    #Spacy Stop Word\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    all_stopwords = sp.Defaults.stop_words\n",
    "    spacy_text = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "    #NLTK STOP WORD\n",
    "    text_stopwords = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    #content = ' '.join(clean)\n",
    "\n",
    "\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    lemmatized_content = []\n",
    "\n",
    "    for word in text_stopwords:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        lemmatized_content.append(lemmatized_word)\n",
    "    \n",
    "    # Joining the lemmatized words back into a string\n",
    "    text = ' '.join(lemmatized_content)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# Define a function to close the window\n",
    "def close():\n",
    "   root.destroy()\n",
    "\n",
    "\n",
    "\n",
    "#Create a Menu\n",
    "my_menu= Menu(root)\n",
    "root.config(menu=my_menu)\n",
    "#Add dropdown to the Menus\n",
    "file_menu=Menu(my_menu,tearoff=False)\n",
    "my_menu.add_cascade(label=\"File\",menu= file_menu)\n",
    "file_menu.add_command(label=\"Open\",command=openfile)\n",
    "file_menu.add_command(label=\"Clear\")\n",
    "file_menu.add_command(label=\"Quit\",command=close)\n",
    "\n",
    " \n",
    "# Create a Button\n",
    "btn1 = Button(root, text = 'Click me !', bd = '5', command = root.destroy) \n",
    "btn.grid(row = 0, column = 1)\n",
    "\n",
    "btn2 = Button(root, text = 'Click me !', bd = '5', command = root.destroy) \n",
    "btn.grid(row = 0, column = 1)\n",
    "\n",
    "root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d90a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebeae8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd46370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261052fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
